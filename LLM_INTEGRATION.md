# LLM-Enhanced Recipe Search üß†

This system uses powerful Language Models (LLMs) to provide intelligent recipe search with:

## Features ‚ú®

### 1. **Smart Query Understanding**
- Understands natural language in multiple languages (English, Hindi, Regional)
- Detects intent, ingredients, dietary restrictions automatically
- Handles complex queries like "jain recipes" (auto-excludes onion/garlic)

### 2. **Multilingual Translation**
- Translate queries between English, Hindi, and regional languages
- Preserves food context and ingredient names
- Provides alternatives in parentheses for clarity

### 3. **Intelligent Ingredient Extraction**
- Extracts explicit ingredients mentioned
- Identifies implied ingredients (e.g., "biryani" implies rice, spices)
- Understands dietary restrictions (jain, vegan, etc.)
- Detects negative ingredients ("without X", "no Y")

### 4. **Automatic Fallback**
- Works with or without LLM API keys
- Falls back to rule-based parsing if LLM unavailable
- No degradation in core functionality

## Setup üöÄ

### 1. Choose Your LLM Provider

We support three providers (in order of recommendation):

#### **DeepSeek R1** (Recommended - Most Cost-Effective)
- **Cost**: $0.14 input / $0.55 output per 1M tokens
- **Quality**: Excellent for recipe understanding
- **Speed**: Very fast
- **Get Key**: https://platform.deepseek.com

#### **OpenAI GPT-4o-mini** (Fallback)
- **Cost**: $0.15 input / $0.6 output per 1M tokens
- **Quality**: Excellent, well-known
- **Speed**: Fast
- **Get Key**: https://platform.openai.com

#### **Gemini 2.0 Flash via Groq** (Third Option)
- **Cost**: $0.075 input / $0.3 output per 1M tokens
- **Quality**: Good, very fast
- **Speed**: Fastest
- **Get Key**: https://console.groq.com

### 2. Add API Key to Environment

**Option A: Using .env file (Recommended)**
```bash
# Copy example file
cp .env.example .env

# Edit .env and add your key
DEEPSEEK_API_KEY=sk-your-actual-key-here
```

**Option B: Using System Environment Variables**

**Windows PowerShell:**
```powershell
$env:DEEPSEEK_API_KEY="sk-your-actual-key-here"
```

**Windows Command Prompt:**
```cmd
set DEEPSEEK_API_KEY=sk-your-actual-key-here
```

**Linux/Mac:**
```bash
export DEEPSEEK_API_KEY=sk-your-actual-key-here
```

### 3. Start Backend
```bash
cd app/api
python main.py
```

You should see:
```
‚úÖ LLM Service initialized with DeepSeek (deepseek-chat)
üß† Enhanced Query Parser initialized
   LLM Mode: ENABLED
```

## API Endpoints üõ†Ô∏è

### 1. Search with LLM Understanding
```bash
GET /api/search?q=jain recipes without onion
GET /api/search?q=pyaz aur lahsun ke bina sabzi
GET /api/search?q=quick pasta under 20 minutes
```

**Response includes:**
- Recipes matching query
- Language detection
- Translated query
- Extracted ingredients
- Dietary preferences detected

### 2. Translate Recipe Text
```bash
GET /api/translate?text=chicken curry&target_lang=Hindi
GET /api/translate?text=‡§Æ‡•Å‡§∞‡•ç‡§ó‡•Ä ‡§ï‡§∞‡•Ä&target_lang=English
```

**Response:**
```json
{
  "original": "chicken curry",
  "translated": "‡§Æ‡•Å‡§∞‡•ç‡§ó‡•Ä ‡§ï‡§∞‡•Ä (chicken curry)",
  "target_language": "Hindi"
}
```

### 3. Analyze Query
```bash
GET /api/analyze?q=jain recipes without tomatoes
```

**Response includes:**
```json
{
  "query": "jain recipes without tomatoes",
  "analysis": {
    "intent": "search",
    "language_detected": "English",
    "dish_name": "jain recipes",
    "excluded_ingredients": ["onion", "garlic", "tomatoes"],
    "dietary_preferences": ["jain"],
    "parsing_method": "LLM"
  },
  "ingredients": {
    "included": [],
    "excluded": ["onion", "garlic", "tomatoes"],
    "implied": ["spices", "vegetables"],
    "dietary_context": "Jain diet excludes root vegetables"
  }
}
```

### 4. Get System Stats
```bash
GET /api/stats
```

**Response:**
```json
{
  "total_recipes": "9600+",
  "search_type": "LLM-Enhanced Semantic Search",
  "llm_enabled": true,
  "llm_provider": "DeepSeek",
  "llm_model": "deepseek-chat"
}
```

## How It Works üîç

### Query Flow

1. **User enters query**: "jain sabzi without tomatoes"

2. **LLM Understanding**:
   - Detects language: English
   - Identifies dietary: Jain (excludes onion, garlic, root vegetables)
   - Finds explicit exclusions: tomatoes
   - Extracts dish type: sabzi (vegetable curry)

3. **Smart Search**:
   - Searches for: "vegetable curry recipes"
   - Filters out: onion, garlic, potato, tomatoes
   - Ranks by relevance

4. **Results**: Perfectly matched Jain-friendly recipes

### Fallback Behavior

**With LLM**:
```python
Query: "pyaz ke bina sabzi"
‚Üí Detects Hindi
‚Üí Translates to: "vegetable curry without onion"
‚Üí Excludes: ["onion"]
‚Üí Smart search
```

**Without LLM** (No API key):
```python
Query: "pyaz ke bina sabzi"
‚Üí Rule-based patterns detect "pyaz" = onion
‚Üí Pattern matches "ke bina" = without
‚Üí Excludes: ["onion"]
‚Üí Basic search
```

Both work, but LLM provides:
- Better language understanding
- Context awareness
- Translation capabilities
- Implied ingredient detection

## Cost Estimation üí∞

**Typical Query Costs (DeepSeek)**:

| Query Type | Tokens | Cost |
|------------|--------|------|
| Simple search | ~500 | $0.00007 |
| Complex analysis | ~1500 | $0.00021 |
| Translation | ~300 | $0.00004 |

**Monthly Costs for 10,000 queries**:
- Simple searches: ~$0.70
- Complex queries: ~$2.10
- Mixed usage: ~$1.50

**Very affordable!**

## Prompt Engineering üìù

The system uses carefully crafted prompts optimized for recipe search:

### Query Understanding Prompt
```
You are an expert food and recipe assistant helping users find recipes.
Analyze natural language queries to extract structured information.

Consider:
- Multiple languages (English, Hindi, regional)
- Dietary restrictions (jain, vegan, vegetarian)
- Cooking methods and times
- Ingredient alternatives
- Cultural context
```

### Translation Prompt
```
Translate recipe-related text preserving:
- Ingredient names and alternatives
- Cooking terminology
- Cultural context
- Food safety considerations
```

## Troubleshooting üîß

### "No LLM API key found"
- Add API key to .env file or environment variable
- System will use rule-based fallback (still works!)

### "LLM API call failed"
- Check API key is correct
- Verify internet connection
- System automatically retries and falls back

### Slow responses
- First query initializes connection (1-2s)
- Subsequent queries use cache (~100ms)
- Consider using Groq for fastest responses

### High costs
- Implement request rate limiting
- Use caching (built-in for 1 hour)
- Consider DeepSeek for lowest costs

## Advanced Configuration ‚öôÔ∏è

### Change LLM Provider Priority

Edit `app/api/llm_config.py`:

```python
PROVIDER_ORDER = [
    LLMProvider.GROQ,      # Try Groq first (fastest)
    LLMProvider.DEEPSEEK,  # Then DeepSeek (cheapest)
    LLMProvider.OPENAI     # Finally OpenAI (most reliable)
]
```

### Adjust Cache TTL

Edit `app/api/llm_service.py`:

```python
self._cache_ttl = 7200  # 2 hours instead of 1
```

### Custom Prompts

Edit prompts in `app/api/llm_config.py`:

```python
SYSTEM_PROMPTS = {
    "query_understanding": "Your custom prompt here...",
    "translation": "Your custom translation prompt...",
    "ingredient_extraction": "Your custom extraction prompt..."
}
```

## Testing üß™

### Test Query Understanding
```bash
curl "http://localhost:8000/api/analyze?q=jain%20recipes%20without%20tomatoes"
```

### Test Translation
```bash
curl "http://localhost:8000/api/translate?text=chicken%20curry&target_lang=Hindi"
```

### Test Search
```bash
curl "http://localhost:8000/api/search?q=quick%20pasta%20under%2020%20minutes"
```

## Performance Metrics üìä

**With LLM**:
- First query: ~800ms (includes LLM call)
- Cached queries: ~50ms (no LLM call)
- Cache hit rate: ~70% typical

**Without LLM**:
- All queries: ~30ms (rule-based only)
- No caching needed
- 100% consistency

## Next Steps üöÄ

1. **Add your API key** to `.env` file
2. **Start backend** with `python main.py`
3. **Test a query** using curl or frontend
4. **Monitor costs** in your LLM provider dashboard
5. **Customize prompts** for your use case

## Support üí¨

- Check logs for detailed information
- Use `/api/stats` to verify LLM status
- System works with or without LLM
- Rule-based fallback always available

---

**Built with ‚ù§Ô∏è for intelligent recipe search**
